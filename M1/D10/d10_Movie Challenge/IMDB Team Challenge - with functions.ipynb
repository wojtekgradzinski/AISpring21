{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "marked-champion",
   "metadata": {},
   "source": [
    "# IMDB Team Challenge\n",
    "\n",
    "It consists of scraping the following information from IMDB:\n",
    "\n",
    "* Movie name\n",
    "* Description\n",
    "* Release Date\n",
    "* Director Name\n",
    "* Rating\n",
    "* Duration\n",
    "* Genre\n",
    "* Stars (Actors)\n",
    "* Filming Dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "electronic-portal",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'seaborn'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-195-db57f7761dbe>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mbs4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBeautifulSoup\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mseaborn\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msns\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'seaborn'"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import math\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "honey-parliament",
   "metadata": {},
   "outputs": [],
   "source": [
    "# request url\n",
    "page = requests.get(\"https://www.imdb.com/search/title/?title_type=feature&num_votes=25000,&genres=adventure&sort=user_rating,desc&start=1\")\n",
    "\n",
    "# create a BeautifulSoup object\n",
    "soup = BeautifulSoup(page.content, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "marked-transcription",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_nums(page_soup):\n",
    "    nums = page_soup.find_all(\"span\",{'class':'lister-item-index'})\n",
    "    nums = [int(i.text.replace('.','')) for i in nums]\n",
    "    return nums\n",
    "#scrape_nums(soup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "guilty-exercise",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_titles(page_soup):\n",
    "    titles_container = page_soup.find_all(class_= \"lister-item-header\")\n",
    "    titles = [title.get_text() for title in titles_container ] #loop to getText in days_container\n",
    "    titles = [title.split('\\n') for title in titles] #split by \\n\n",
    "    titles_names = [title[2] for title in titles] #split by \\n\n",
    "    return titles_names\n",
    "\n",
    "#scrape_titles(soup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ultimate-jungle",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_years(page_soup):\n",
    "    titles_container = page_soup.find_all(class_= \"lister-item-header\")\n",
    "    title_splits = [title.get_text().split('\\n') for title in titles_container ] #loop to getText in days_container\n",
    "    years = [title[3] for title in title_splits]\n",
    "    return years\n",
    "\n",
    "#scrape_years(soup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "parliamentary-china",
   "metadata": {},
   "outputs": [],
   "source": [
    "# - Release Date Scraper\n",
    "\n",
    "def scrape_years(page_soup):\n",
    "    titles_container = page_soup.find_all(class_= \"lister-item-header\")\n",
    "    title_splits = [title.get_text().split('\\n') for title in titles_container ] #loop to getText in days_container\n",
    "    years = [title[3] for title in title_splits]\n",
    "    return years\n",
    "\n",
    "#scrape_years(soup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mighty-caribbean",
   "metadata": {},
   "outputs": [],
   "source": [
    "# - Director Scraper\n",
    "\n",
    "def scrape_dirs(page_soup):\n",
    "    movies_director=page_soup.find_all(class_= \"lister-item-content\")\n",
    "    director_list=[]\n",
    "    for director in movies_director:\n",
    "        director_list.append(director.find(\"p\",{\"class\":\"\"}).a.text)\n",
    "    return director_list\n",
    "\n",
    "#scrape_dirs(soup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "authentic-uncle",
   "metadata": {},
   "outputs": [],
   "source": [
    "# - Rating Scraper\n",
    "\n",
    "def scrape_ratings(page_soup):\n",
    "    movies_rating=page_soup.find_all(class_=\"inline-block ratings-imdb-rating\")\n",
    "    rating_list=[]\n",
    "    for rating in movies_rating:\n",
    "        rating_list.append(int(rating.strong.text[2])*0.1+8)\n",
    "    return rating_list\n",
    "\n",
    "#scrape_ratings(soup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "powerful-beginning",
   "metadata": {},
   "outputs": [],
   "source": [
    "# - Duration Scraper\n",
    "\n",
    "def scrape_durs(page_soup):\n",
    "    duration = soup.find_all(class_='lister-item-content')\n",
    "    dur = [title.get_text() for title in duration]\n",
    "    dur = [title.split('\\n') for title in dur]\n",
    "    #display(dur)\n",
    "    duration = [title[9] for title in dur]\n",
    "    # display(duration)\n",
    "    \n",
    "    duration_list=[]\n",
    "    for i in (duration):\n",
    "        i.split()\n",
    "        duration_list.append(int(i[:-3]))\n",
    "\n",
    "\n",
    "\n",
    "    return duration_list\n",
    "#scrape_durs(soup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pregnant-column",
   "metadata": {},
   "outputs": [],
   "source": [
    "# - Genres Scraper\n",
    "\n",
    "def scrape_genres(page_soup):\n",
    "    genres = soup.find_all(\"span\",{'class':'genre'})\n",
    "    genres = [i.text.strip().split(',') for i in genres]\n",
    "    return genres\n",
    "#scrape_genres(soup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "regulated-lingerie",
   "metadata": {},
   "outputs": [],
   "source": [
    "# - Stars Scraper\n",
    "def scrape_stars(soup_page):\n",
    "    movies_stars=soup_page.find_all(class_= \"lister-item-content\")\n",
    "    stars_list=[]\n",
    "    for stars in movies_stars:\n",
    "        stars_list.append(str(stars.find(\"p\",{\"class\":\"\"}).text.split(\"\\n\")[5:-1]).strip())\n",
    "    return stars_list\n",
    "\n",
    "#scrape_stars(soup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fourth-edition",
   "metadata": {},
   "outputs": [],
   "source": [
    "movies_descs=soup.find_all(class_= \"lister-item-content\")\n",
    "\n",
    "x=movies_descs[0].find_all(\"p\",{\"class\":\"text-muted\"})\n",
    "x[1].text.split(\"\\n\")[1].strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "advisory-expense",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get descriptions\n",
    "def scrape_descs(soup_page):\n",
    "    movies_descs=soup_page.find_all(class_= \"lister-item-content\")\n",
    "    descs_list=[]\n",
    "    #movies_descs.findall(\"p\",{\"class\":\"text-muted\"})\n",
    "    for descs in movies_descs:\n",
    "        descs_list.append((descs.find_all(\"p\",{\"class\":\"text-muted\"}))[1].text.split(\"\\n\")[1].strip())\n",
    "    return descs_list\n",
    "\n",
    "#scrape_descs(soup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "prime-corps",
   "metadata": {},
   "outputs": [],
   "source": [
    "#function that takes how many movies you want to scrape and returns ....\n",
    "# request url\n",
    "page = requests.get(\"https://www.imdb.com/search/title/?title_type=feature&num_votes=25000,&genres=adventure&sort=user_rating,desc&start=1\")\n",
    "\n",
    "# create a BeautifulSoup object\n",
    "soup = BeautifulSoup(page.content, 'html.parser')\n",
    "\n",
    "\n",
    "def IMDB_scraper2(num_mov):\n",
    "    num_pages = num_mov/2\n",
    "    print(\"Will scare {} movies over {} pages.\".format(num_mov,num_pages))\n",
    "\n",
    "    nums, titles, descs, years, dirs, ratings, durs, genres, stars, film_dates = [],[],[],[],[],[],[],[],[],[]\n",
    "\n",
    "    for i in range (0,int(num_pages)):\n",
    "        ind = i*50+1\n",
    "        page = requests.get(\"https://www.imdb.com/search/title/?title_type=feature&num_votes=25000,&genres=adventure&sort=user_rating,desc&start={}\".format(ind))\n",
    "\n",
    "        page_soup = BeautifulSoup(page.content, 'html.parser')\n",
    "\n",
    "        #Add scraping scripts here\n",
    "        #nums.append(scrape_nums(page_soup))\n",
    "        #titles.append(scrape_titles(page_soup))\n",
    "        #descs.append(scrape_descs(page_soup))\n",
    "        #years.append(scrape_years(page_soup))\n",
    "        #dirs.append(scrape_dirs(page_soup))\n",
    "        #ratings.append(scrape_ratings(page_soup))\n",
    "        #durs.append(scrape_durs(page_soup))\n",
    "        #genres.append(scrape_genres(page_soup))\n",
    "        #stars.append(scrape_stars(page_soup))\n",
    "        #film_dates.append(scrape_film_dates(page_soup))\n",
    "\n",
    "        #check len\n",
    "        #if any(len(lst) != length for lst in [titles, descs, years, dirs, ratings, durs, genres, stars, film_dates]):\n",
    "        #    print('different number of scraped elements on page {}!'.format(i+1))\n",
    "        #    break\n",
    "\n",
    "    \n",
    "    \n",
    "    #print(descs[0])\n",
    "    #print(years[0])\n",
    "    #print(ratings[0])\n",
    "    #print(durs[0])\n",
    "    #print(genres[0])\n",
    "    #print(stars[0])\n",
    "\n",
    "    scrape_df = pd.DataFrame({'Number': scrape_nums(page_soup),\n",
    "                    'Title': scrape_titles(page_soup), \n",
    "                    'Description': scrape_descs(page_soup),\n",
    "                    'Release date':scrape_years(page_soup),\n",
    "                    'Director':scrape_dirs(page_soup),\n",
    "                    'Rating':scrape_ratings(page_soup),\n",
    "                    'Duration':scrape_durs(page_soup),\n",
    "                    'Genre':scrape_genres(page_soup),\n",
    "                    'Stars':scrape_stars(page_soup),\n",
    "                    #'Filming Date':film_dates\n",
    "                    })\n",
    "\n",
    "    return scrape_df\n",
    "\n",
    "\n",
    "IMDB_scraper2(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "brief-daily",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=IMDB_scraper2(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "given-consistency",
   "metadata": {},
   "outputs": [],
   "source": [
    "#normalizing duration max/min\n",
    "df[\"Duration\"] = (df[\"Duration\"] - df[\"Duration\"].min()) / (df[\"Duration\"].max() - df[\"Duration\"].min())\n",
    "#df[\"Duration\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "desperate-walnut",
   "metadata": {},
   "outputs": [],
   "source": [
    "#normalizing rating max/min\n",
    "df[\"Rating\"] = (df[\"Rating\"]  - df[\"Rating\"] .min()) / (df[\"Rating\"] .max() - df[\"Rating\"] .min())\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "third-progress",
   "metadata": {},
   "outputs": [],
   "source": [
    "#normalizing rating (mean)\n",
    "df[\"Rating\"] = (df[\"Rating\"] - df[\"Rating\"].mean()) / (df[\"Rating\"].max() - df[\"Rating\"].min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "quality-offering",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.subplots(figsize=(12.5,4))\n",
    "#plt.xlim(-0.2,1.2)\n",
    "#plt.ylim(-0.2,1.2)\n",
    "plt.scatter(df['Title'],df['Director'],s=70,color=\"r\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
